name: Scrape Cardrush Buying Prices

on:
  workflow_dispatch:
  schedule:
    - cron: "17 */3 * * *"

permissions:
  contents: write

jobs:
  scrape:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install dependencies
        run: pip install -r one_piece_scripts/requirements.txt

      - name: Diagnose and clear proxy environment
        run: |
          echo "Proxy-related environment variables before cleanup:"
          env | grep -Ei '(^|_)(http|https|all|no)_proxy=' || true

          unset HTTP_PROXY HTTPS_PROXY ALL_PROXY NO_PROXY
          unset http_proxy https_proxy all_proxy no_proxy

          echo "Proxy-related environment variables after cleanup:"
          env | grep -Ei '(^|_)(http|https|all|no)_proxy=' || true

      - name: Decide whether today's scrape is needed
        env:
          TZ: Asia/Tokyo
        run: |
          month_year_dir=$(date +"%b %Y")
          dated_json=$(date +"%Y-%m-%d.json")
          output_dir="one_piece_scripts/cardrush_buying_prices/${month_year_dir}"
          output_path="${output_dir}/${dated_json}"

          mkdir -p "$output_dir"
          echo "SCRAPE_OUTPUT_PATH=$output_path" >> "$GITHUB_ENV"

          if [ ! -f "$output_path" ]; then
            echo "No scrape output for today yet; scraping now."
            echo "SHOULD_SCRAPE=true" >> "$GITHUB_ENV"
            exit 0
          fi

          echo "Found today's scrape output at $output_path; validating JSON..."
          if python - "$output_path" <<'PY'
          import json
          import sys

          path = sys.argv[1]
          with open(path, 'r', encoding='utf-8') as f:
              data = json.load(f)

          if not isinstance(data, list):
              raise SystemExit("JSON root is not a list")

          if len(data) == 0:
              raise SystemExit("JSON list is empty")

          print(f"Validated JSON list with {len(data)} rows")
          PY
          then
            echo "Today's JSON is already valid; skipping scrape."
            echo "SHOULD_SCRAPE=false" >> "$GITHUB_ENV"
          else
            echo "Today's JSON exists but is invalid/empty; scraping again."
            echo "SHOULD_SCRAPE=true" >> "$GITHUB_ENV"
          fi
      - name: Scrape Cardrush paginated list
        if: env.SHOULD_SCRAPE == 'true'
        env:
          HTTP_PROXY: ""
          HTTPS_PROXY: ""
          ALL_PROXY: ""
          NO_PROXY: "cardrush.media"
        run: |
          set -o pipefail

          echo "Network diagnostics:"
          python - <<'PY'
          import os, socket
          print("HTTP_PROXY=", os.getenv("HTTP_PROXY"))
          print("HTTPS_PROXY=", os.getenv("HTTPS_PROXY"))
          print("ALL_PROXY=", os.getenv("ALL_PROXY"))
          print("NO_PROXY=", os.getenv("NO_PROXY"))
          print("cardrush.media resolves to:", socket.gethostbyname("cardrush.media"))
          PY

          python one_piece_scripts/scrape_cardrush_buying_prices.py \
            --output "$SCRAPE_OUTPUT_PATH" 2>&1 | tee scrape-cardrush.log

      - name: Upload scrape logs on failure
        if: failure() && env.SHOULD_SCRAPE == 'true'
        uses: actions/upload-artifact@v4
        with:
          name: scrape-cardrush-log
          path: scrape-cardrush.log

      - name: Commit updated scrape output
        if: env.SHOULD_SCRAPE == 'true'
        run: |
          if [ ! -f "$SCRAPE_OUTPUT_PATH" ]; then
            echo "Expected output file was not created: $SCRAPE_OUTPUT_PATH"
            exit 1
          fi

          git config user.name "github-actions[bot]"
          git config user.email "41898282+github-actions[bot]@users.noreply.github.com"
          git add "$SCRAPE_OUTPUT_PATH"

          if git diff --cached --quiet -- "$SCRAPE_OUTPUT_PATH"; then
            echo "No changes to commit"
            exit 0
          fi

          git commit -m "chore: add dated cardrush buying prices data"
          git push
